# The linear model is considered as the null hypothesis

#Importing the required modules
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy import optimize, stats
import csv

# Converting the dat file to csv file
# read testdata.dat to a list of lists
datContent = [i.strip().split() for i in open("/home/siddhant/Downloads/testdata.dat.txt").readlines()]

# write it as a new CSV file
with open("./testdata.csv", "w") as f:
    writer = csv.writer(f)
    writer.writerows(datContent)
    
#read the csv file
global data, x, y, sigma_y, val1, val2, val3  #defined as global so all functions can access these values
data = pd.read_csv('testdata.csv')
x = data['x']
y = data['y']
sigma_y = data['sigma_y']

#initialise theta for all the fits to zero
v_1 = np.array([0, 0])
v_2 = np.array([0, 0, 0])
v_3 = np.array([0, 0, 0, 0])

# Defining the required functions for fitting
def linear(x, v_1):     #linear
    return v_1[1]*x+v_1[0]

def quadratic(x, v_2):      #quadratic
    return v_2[2]*x**2 + v_2[1]*x + v_2[0]

def cubic(x, v_3):       #cubic
    return v_3[3]*x**3 + v_3[2]*x**2 + v_3[1]*x + v_3[0]

#The log-likelihood function
#The log-likelihood function is maximum for best fitting value of theta

def logL(theta, n):
    if n==1:
        y_fit = linear(x, theta)
    elif n==2:
        y_fit = quadratic(x, theta)
    elif n==3:
        y_fit = cubic(x, theta)
    return sum(stats.norm.logpdf(*args)
               for args in zip(y, y_fit, sigma_y))

#This function returns the best theta value for the fitting
def best_theta(n, theta_val):
    if n==1:
        theta_0 = (n+1)*[0]
        neg_logL = lambda theta: -logL(theta, 1)
        return optimize.fmin_bfgs(neg_logL, theta_0, disp=False)
    if n==2:
        theta_0 = (n+1)*[0]
        neg_logL = lambda theta: -logL(theta, 2)
        return optimize.fmin_bfgs(neg_logL, theta_0, disp=False)
    if n==3:
        theta_0 = (n+1)*[0]
        neg_logL = lambda theta: -logL(theta, 3)
        return optimize.fmin_bfgs(neg_logL, theta_0, disp=False)

    
    
#Compute chi2 likelihood for frequentist
def compute_chi2(n):
    if n==1:
        theta = best_theta(n, v_1)
        resid = ((y - linear(x, theta)) / sigma_y)
    elif n==2:
        theta = best_theta(n, v_2)
        resid = ((y - quadratic(x, theta)) / sigma_y)
    elif n==3:
        theta = best_theta(n, v_3)
        resid = ((y - cubic(x, theta)) / sigma_y)

    return np.sum(resid ** 2)

#Computing degree of freedom
def compute_dof(degree, data=data):
    return data.shape[0] - (degree + 1)

#Computing chi square likelihood
def chi2_likelihood(n):
    chi2 = compute_chi2(n)
    dof = compute_dof(n)
    return stats.chi2(dof).pdf(chi2)

#Compute the p value for the fit using linear model as the null hypothesis
def p_val(n):
    return 1-stats.chi2(n-1).cdf(compute_chi2(1) - compute_chi2(n))

#Compute the optimized values of the parameters
r1 = best_theta(1, v_1)
r2 = best_theta(2, v_2)
r3 = best_theta(3, v_3)

#Compute the AIC values using AIC = -2*log likelihood(optimized value of theta,n)+2*(n+1)
def aic(r1,n):
    return -2*logL(r1,n)+2*(n+1)

#Compute the AIC values corrected for small sample size which is AICc 
#AICc values = AIC value + 2*k*(k+1)/(N-K-1)
def aic_c(r1,n,N):
    k=n+1
    return aic(r1,n) + (2*k*(k+1))/(N-k-1)

#function calling & calculating AICc for different models
AIC1 = aic_c(r1,1,20)
AIC2 = aic_c(r2,2,20)
AIC3 = aic_c(r3,3,20)

#Compute the BIC values
#Calculatig BIC = -2*log likelihood + 2*numpy.log(sample size)
BIC1 = -2*logL(r1, 1) + 2*np.log(x.shape[0])
BIC2 = -2*logL(r2, 2) + 3*np.log(x.shape[0])
BIC3 = -2*logL(r3, 3) + 4*np.log(x.shape[0])

#Computing delta AIC
AIC_min = min(AIC1, AIC2, AIC3)

#Computing delta BIC
BIC_min = min(BIC1, BIC2, BIC3)

################################### functions ended ###########################


#################### print functions ###########################

#Printing log likelihood values for different models
print("Log L values")
print("linear model: logL =", logL(best_theta(1, v_1), 1))
print("quadratic model:logL =", logL(best_theta(2, v_2), 2))
print("cubic model: logL =",logL(best_theta(3, v_3), 3))

#Printing chi square likelihood values for different models
print("chi2 likelihood")
print("- linear model:    ", chi2_likelihood(1))
print("- quadratic model: ", chi2_likelihood(2))
print("- cubic model: ", chi2_likelihood(3))

#Printing p values values for different models except linear
#The p value for null hypothesis will not be defined as the delta chi square value is zero
print("p_values")
print("- quadratic model: ", p_val(2))
print("- cubic model: ", p_val(3))

"""Bayesian Analysis"""

#Printing AIC values for different models
print("AIC values")
print("- linear model:    ", AIC1)
print("- quadratic model: ", AIC2)
print("- cubic model: ", AIC3)

#Printing delta AIC values for different models
print("Delta AIC values")
print("- linear model:    ", AIC1-AIC_min)
print("- quadratic model: ", AIC2-AIC_min)
print("- cubic model: ", AIC3-AIC_min)

#Interpreting results for AIC
print("For quadratic model, 0 < delta AIC < 2, which implies that quadratic model also has a substantial support")
print("For cubic model, 2 < delta AIC < 4, which implies that cubic model has cosiderably less support")
print(" For linear model, delta AIC = 0, which implies that model has substantial support")

#Printing BIC values for different models
print("BIC vlaues")
print("- linear model:    ", BIC1)
print("- quadratic model: ", BIC2)
print("- cubic model: ", BIC3)

#Printing delta BIC values for different models
print("Delta BIC values")
print("- linear model:    ", BIC1-BIC_min)
print("- quadratic model: ", BIC2-BIC_min)
print("- cubic model: ", BIC3-BIC_min)

#Interpreting results for BIC
print("For quadratic model, 0 < delta BIC < 2, which implies that there is no evidence against quadratic model")
print("For cubic model, 2 < delta AIC < 4, which implies that there is positive evidence against cubic model")
print(" For linear model, delta BIC = 0, which implies that model cannot be rejected")

#Compring AIC values to find best fit model
if AIC1 < AIC2 and AIC1 < AIC3:
        print("Linear Model is the best fit based on AIC.")
elif AIC2 < AIC1 and AIC2 < AIC3:
        print("Quadratic Model is the best fit based on AIC.")
elif AIC3 < AIC1 and AIC2 < AIC3:
        print("Cubic Model is the best fit based on AIC.")   

if BIC1 < BIC2 and BIC1 < BIC3:
        print("Linear Model is the best fit based on BIC.")
elif BIC2 < BIC1 and BIC2 < BIC3:
        print("Quadratic Model is the best fit based on BIC.")
elif BIC3 < BIC1 and BIC2 < BIC3:
        print("Cubic Model is the best fit based on BIC.")   


############# Plotting graphs using different line styles #####################
t =  np.linspace(0, 1, 1000)
fig, ax = plt.subplots(figsize=(10, 10))
plt.plot(t, linear(t, r1), label='linear_fitting')
plt.plot(t, quadratic(t, r2), label='quadratic_fitting')
plt.plot(t, cubic(t, r3), label='cubic_fitting')
plt.legend()
plt.xlabel('$x$')
plt.ylabel('$y$')
plt.title("Curve fitting using Linear, Quadratic and Cubic Models", size=15)
ax.errorbar(x, y, sigma_y, fmt='ok', ecolor = 'gray')
plt.show()
